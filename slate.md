**Trends in the Threat Landscape (Agenda)**

1. AI Makes Social Engineering More Effective

2. Ransomware Continues to Mature

3. Third-Parties Can Open the Door to Attackers

4. Our Digital Inter-dependencies Amplify Cyber Attacks

5. Our Predictions for 2026

**Trend 1: AI Takes Social Engineering to the Next Level**

a. Threat actors are using generative AI to create convincing deepfakes, emails, and voice clones that make fraud nearly impossible to spot.
b. The line between real and synthetic communication is disappearing, putting brand trust and internal verification under pressure.
c. Organizations will need stronger identity validation and human oversight as attackers learn to mimic not just messages, but people.

Phishing emails have already increased 1,265% since the launch of ChatGPT.

**Arup Deepfake Incident: When AI Speaks for You**

Attackers impersonated a senior Arup executive using an AI-generated deepfake during live video meetings. The fake executive instructed teams to transfer project funds and share sensitive data.
Public video/voice samples were used to train a generative AI model capable of real-time facial and voice synthesis. The deepfake was delivered over legitimate conferencing tools.
Arup suffered financial losses of around $25 million, temporary project disruptions and reputational damage. The breach exposed the growing risk of deepfake-driven social engineering.

What Went Wrong?
Employees trusted video identity as proof of authenticity.
No formal verification for financial or data-sharing instructions.
Awareness training hadn’t covered AI-driven impersonation.

Lessons Learned
Add secondary verification steps for executive approvals.
Educate staff on recognizing behavioural or contextual inconsistencies.
Deploy deepfake-detection and media integrity tools.
Integrate deepfake scenarios into fraud and incident-response playbooks.

**Replit AI Agent: Autonomous Code Turned Destructive**

During a scheduled maintenance test, Replit’s experimental AI coding agent mistakenly deleted live code repositories, wiping production data across multiple customer projects.
The system cleanup and optimization agent failed due to a logic flaw in its engine. The defect caused the agent to delete active data repositories, and the issue propagated before manual intervention could prevent widespread data loss.
Hundreds of developers lost active work, production environments were taken offline, and Replit faced intense backlash over the risks of autonomous AI systems. 

What Went Wrong?
The AI agent operated with unsupervised, broad privileges.
No human approval loop or failsafe to prevent destructive actions.
Limited environment separation between test and production systems.
Governance controls focused on data privacy, not AI autonomy.

Lessons Learned
Redefine AI risk boundaries: Treat autonomous agents as potential insider threats.
Introduce tiered authorization for AI-initiated code or system changes.
Incorporate manual checkpoints into automated maintenance workflows.
Expand governance to include AI model behavior and access reviews.

